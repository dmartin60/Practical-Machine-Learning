---
output:
  html_document:
    keep_md: yes
  pdf_document: default
  word_document: default
---

##Exercise Effectiveness Predictive Model ##
  
  Don Martin                      
  March 5, 2017
  
###Executive Summary###

Fitness tracking devices such as Jawbone Up, Nike Fuel Band, and Fitbit, enable people to inexpensively collect personal activity data, then find and change patterns in personal behavior to improve health. These devices can make it easy to quantify **how much** of a particular activity is completed, but rarely do they quantify **how well** activities are performed.  In our study six participants were asked to perform barbell lifts correctly and incorrectly five different ways. Observations were collected from accelerometers on each participant's belt, forearm, arm, and dumbbell. *Participants were asked to perform one set of 10 repetitions of the Unilateral Dumbbell Biceps Curl in five different fashions: exactly according to the specification (Class A), throwing the elbows to the front (Class B), lifting the dumbbell only halfway (Class C), lowering the dumbbell only halfway (Class D) and throwing the hips to the front (Class E).* This project leverages this accelerometer data to predict the manner in which the barbell lift exercises are performed.

###Data Processing###
**Data Acquisition**

Accelerometer data of 6 participants for this project will be obtained from: http://groupware.les.inf.puc-rio.br/har. The objective is to create a model to predict the manner in which participants performed barbell lift exercises as captured in the training set pml-training.csv **"classe"** variable. The code below is a repeatable process for obtaining and loading the study data.

```{r}

  
#   Establish & Create Data Directory
    if (!file.exists("./data")) { dir.create("./data") }

#   Retrieve and Load Training DataSet
    trainURL  <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
    trainCSV  <- "./data/pml-training.csv"
    if (!file.exists(trainCSV)) {download.file( trainURL, destfile=trainCSV)}
    trainDf   <- read.csv(trainCSV, na.strings=c("NA",""), header=TRUE)

#   Retrieve and Load Testing DataSet
    testURL   <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
    testCSV   <- "./data/pml-testing.csv"
    if (!file.exists(testCSV)) {download.file( testURL, destfile=testCSV)}
    testDf    <- read.csv(testCSV, na.strings=c("NA",""), header=TRUE)
```

On initial inspection the training data set loaded into data frame **trainDF** contains 160 variables and 19622 observations. The testing data set, loaded into data frame **testDf** contains 160 variables and 20 observations. One notable difference is **trainDF** contains the **"classe"** variable, the target outcome describing the manner in which exercises were performed. The test data set **testDf** contains problem_id (int) instead of classe. 

This study will use the **trainDF** to train and validate different model(s). Cross-validation within the training partition of **trainDF** will be leveraged to improve model fit. The best model will be selected based on accuracy and performance. Finally, an out-of-sample test using the test data provided **testDf**  will be used to evaluate forecasting accuracy and performance of the selected model.

**Data Cleansing**

This section illustrates steps taken to prepare both train and test data. The data is cleansed, removing the variables(columns) that contain NA or missing values, or do not contribute to accelerometer measurements. As the **testDf** does not contain the "classe" variable, it will be set aside as the out-of-sample  **testDS** in this step.

```{r}
  InputDf  <- trainDf  # cleanse the Training Dataframe
#   Step 1. Remove Variables containing missing or NA values.
    InputDf <- trainDf
    Step01  <- InputDf[, colSums(is.na(InputDf)) == 0]
     
#   Step 2. Remove timestamp and window variables that do not describe accelerometer measurements
    target  <- grepl("^X|timestamp|window", names(Step01))
    Step02  <- Step01[, !target]
     
#   Step 3. Coerce accelerometer measurements (variables) to be numeric
    classe        <- Step02$classe   #Preserve the classe (factor) variable 
    Step03        <- Step02[, sapply(Step02, is.numeric)]
    Step03$classe <- classe   #Join classe to the remaining dataset
    inTrainDS     <- Step03
    
  InputDf  <- testDf # cleanse the Testing Dataframe (using the same steps)
#   Step 1. Remove Variables containing missing or NA values.
    Step01  <- InputDf[, colSums(is.na(InputDf)) == 0]
     
#   Step 2. Remove timestamp and window variables that do not describe accelerometer measurements
    target  <- grepl("^X|timestamp|window", names(Step01))
    Step02  <- Step01[, !target]
     
#   Step 3. Coerce accelerometer measurements (variables) to be numeric - classe doesn't exist in test
    Step03  <- Step02[, sapply(Step02, is.numeric)]
    testDS  <- Step03
```

Data cleansing has removed 107 null or un-interesting variables from each data set.  The training data set **inTrainDS** contains 53 variables and 19622 observations. The cleansed testing data set **testDS** contains 53 variables and 20 observations.  

**Data Slicing**

This section illustrates steps taken to split the training data set **inTrainDS** into a model training data set **trainDS** (60%) and a model validation data set **validationDS** (40%).

```{r results='hide', message=FALSE, warning=FALSE}
# Load required packages
  library(caret)
 
  
  library(rattle)
  library(rpart.plot)
  library(RColorBrewer)
 

  set.seed(12345)  
  trainPar <- createDataPartition(inTrainDS$classe, p=0.60, list=F)
  trainingDS  <- inTrainDS[trainPar, ]   # 60%
  validationDS   <- inTrainDS[-trainPar, ]  # 40%
```

Data slicing split the data set **inTrainDS** into **trainingDS** containing 53 variables, 11776 observations, and **validationDS** containing 53 variables, 7846 observations.  

###Model Creation & Selection###

**Training Models using 5-fold cross-validation **

Two differnt model algorithms are trained below, Decision trees with CART (rpart), for its ease and peformance advantages, and Random forest decision trees (rf) for its accuracy.  Both models are fit on **trainingDS**.  A 5-fold cross-validation is used to select optimal tuning parameters for each algorithm.

```{r results='hide', message=FALSE, warning=FALSE}

# Use 5-fold cross-validation to select optimal tuning parameters
  ctlRf <- trainControl(method="cv", 
                        number=5, 
                        verboseIter=F)
```

Decision trees with CART (rpart), is trained and graphed below.

```{r results='hide', message=FALSE, warning=FALSE}
# Train Decision trees with CART (rpart)
  fitRp <- train(classe ~ ., 
                 data=trainingDS, 
                 method="rpart", 
                 trControl=ctlRf) 
# fitRp
  plot(fitRp, Main="Decision trees with CART")
```

The accuracy of the optimal *decision trees with CART* algorithm is **0.4950918** or **49%**. Next Random forest decision trees is trained using the same **trainingDS** and graphed below.

```{r results='hide', message=FALSE, warning=FALSE}
# Train Random forest decision trees (rf)
  fitRf <- train(classe ~ ., 
                 data=trainingDS, 
                 method="rf", 
                 trControl=ctlRf, 
                 ntree=250)
# fitRf
  plot(fitRf, Main="Random forest")
```

The accuracy of the optimal *random forest decision trees* algorithm **0.988450** of **99%**.  Based on the difference in accuracy, the performance trade-off of the random forest is acceptable, and the **fitRf** model is selected.  

**Model Assessment **

In this section we assess the Random forest model's accuracy in predicting the manner participants performed barbell lift exercises as captured by the classe variable in **validationDS**.

```{r}
# Validate Random forest decision trees (rf)
  predictRf <- predict(fitRf, validationDS)
  cmatrixRf <-confusionMatrix(validationDS$classe, predictRf)
  os_error  <- 1 - as.numeric(confusionMatrix(validationDS$classe, predictRf)$overall[1])
  
  cmatrixRf
  os_error
```

The estimated accuracy of the model is **0.9920979** or **99%** and the estimated out-of-sample error is **0.007902116** or **.79%**.  Below is a list of the important predictors for the model.

```{r}
# Validate Random forest decision trees (rf)
  varImp(fitRf)
```

###Predicting for Test Data Set###

Finally we apply the the Random forest model **fitRf** to predict the manner participants performed barbell lift exercises for each of the 20 observations based on the accelerometer variables contained in the test data sample ('pml-testing.csv').
 
```{r}
# Predict using Random forest decision trees (rf)

  predictRf <- predict(fitRf, newdata=testDS)
  PredictionResults <- data.frame(problem_id=testDS$problem_id,
                                  predicted=predictRf)
  print(PredictionResults)
```
 
###Conclusion###

Given the data provided, the random forest model with cross-validation produces a surprisingly accurate model that sufficient to predict the manner participants performed barbell lift exercises in the study.

